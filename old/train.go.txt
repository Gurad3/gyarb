package main

import (
	"fmt"
)

func (net *netWork) TrainSplit(data [][]float64, expectedData [][]float64, testData [][]float64, exTest [][]float64) {
	batchSize := 100
	i := 0
	fmt.Println(net.runNet(data[0]))
	for true {
		for dataIndex, dataSlice := range data {

			net.Train(dataSlice, expectedData[dataIndex])

			if i%batchSize == 0 {
				net.applyGrad(float64(batchSize))
			}
			//net.applyGrad(float64(1))
			if i%(batchSize*1000) == 0 {

				fmt.Println("Performance on 100 first TrainData: ", net.TestPerformance(data, expectedData, 100))
				fmt.Println("Performance on Test Data", net.TestPerformance(testData, exTest, len(testData)))

				net.SaveToFile()
			}
			i++
		}
	}
}

func (net *netWork) TestPerformance(data [][]float64, expectedData [][]float64, ln int) float64 {
	score := 0
	i := 0
	for true {
		for dataIndex, dataSlice := range data {
			if i >= ln {
				return float64(score) / float64(ln)
			}
			out := net.runNet(dataSlice)
			if checkAnswer(out, expectedData[dataIndex]) {
				score += 1
			}
			i++
		}
	}
	return 0
}
func checkAnswer(out []float64, ex []float64) bool {
	right := 0
	for pos, val := range ex {
		if val == 1 {
			right = pos
			break
		}
	}

	p := 0
	hig := float64(0)
	for i, val := range out {
		if val > hig {
			p = i
			hig = val
		}
	}
	return p == right
}

func (net *netWork) Train(data []float64, expectedData []float64) {
	net.runNet(data)
	net.backProp(expectedData)
	//Run net.ApplayGrad, dock efter hela batchen
}

func (net *netWork) updateGrad(nodeVals []float64, layerID int, prevLayAkt []float64) {
	lay := net.layers[layerID]
	for neuronID := range nodeVals {

		for weightID := range lay.weights[neuronID] {
			//fmt.Println(len(lay.weightsAdd), len(nodeVals), len(prevLayAkt))
			lay.weightsAdd[neuronID][weightID] += nodeVals[neuronID] * prevLayAkt[weightID]
		}

		lay.biasAdd[neuronID] += nodeVals[neuronID]
	}
}

func (net *netWork) backProp(target []float64) {

	outLayer := net.layers[len(net.layers)-1]
	nodeValues := make([]float64, len(outLayer.out))
	for neuronID := range outLayer.out {
		addVal := devGetCost(outLayer.out[neuronID], target[neuronID]) * outLayer.aktDevFunc(outLayer.nonAktOut[neuronID])
		nodeValues[neuronID] = addVal
	}
	net.updateGrad(nodeValues, len(net.layers)-1, net.layers[len(net.layers)-2].out)

	for layerID := len(net.layers) - 2; layerID >= 0; layerID-- {
		currLayer := net.layers[layerID]
		newNodeVals := make([]float64, len(currLayer.out))

		for neuronID := range currLayer.out {
			newVal := float64(0)
			for oldeNodeIndex := range nodeValues {
				newVal += nodeValues[oldeNodeIndex] * net.layers[layerID+1].weights[oldeNodeIndex][neuronID]

			}
			newVal *= currLayer.aktDevFunc(currLayer.nonAktOut[neuronID])
			newNodeVals[neuronID] = newVal
		}

		nodeValues = newNodeVals
		var prevOut []float64
		if layerID == 0 {
			prevOut = net.inp

		} else {
			prevOut = net.layers[layerID-1].out
		}

		net.updateGrad(nodeValues, layerID, prevOut)
	}

}

/*
func (net *netWork) applyGrad(batchSize float64) {
	for _, currLayer := range net.layers {
		for neuronID, neuronWeights := range currLayer.weights {

			for weightID := range neuronWeights {
				neuronWeights[weightID] -= currLayer.weightsAdd[neuronID][weightID] * net.learnRate / batchSize
				currLayer.weightsAdd[neuronID][weightID] = 0
			}

			currLayer.bias[neuronID] -= currLayer.biasAdd[neuronID] * net.learnRate / batchSize
			currLayer.biasAdd[neuronID] = 0
		}

	}
}
*/

func (net *netWork) applyGrad(batchSize float64) {
	for _, currLayer := range net.layers {
		for neuronID, neuronWeights := range currLayer.weights {

			for weightID := range neuronWeights {
				neuronWeights[weightID] -= currLayer.weightsAdd[neuronID][weightID] * net.learnRate / batchSize
				currLayer.weightsAdd[neuronID][weightID] = 0
			}

			currLayer.bias[neuronID] -= currLayer.biasAdd[neuronID] * net.learnRate / batchSize
			currLayer.biasAdd[neuronID] = 0
		}
	}
}

func getCost(neuronVal float64, targetVal float64) float64 {
	er := neuronVal - targetVal
	return er * er
	//return math.Pow(neuronVal-targetVal, 2)
}

func devGetCost(neuronVal float64, targetVal float64) float64 {
	return 2 * (neuronVal - targetVal)
}

func getCostOutPut(netOut []float64, target []float64) float64 {
	cost := float64(0)
	for i, val := range netOut {
		cost += getCost(val, target[i])
	}
	return cost
}

func (net *netWork) getCostBatch(data [][]float64, expected [][]float64) float64 {
	totalCost := float64(0)
	for i, inputs := range data {
		out := net.runNet(inputs)
		totalCost += getCostOutPut(out, expected[i])
	}

	return totalCost / float64(len(data))
}
